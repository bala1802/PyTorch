{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Module.add_module()\n",
    "\n",
    "This is particularly useful when you want to dynamically add sub-modules to your model, especially when the structure of your neural network is determined during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.add_module('linear1', nn.Linear(10, 20))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('linear2', nn.Linear(20, 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=10, out_features=20, bias=True),\n",
       " Linear(in_features=20, out_features=5, bias=True),\n",
       " ReLU())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_1 = model.linear1\n",
    "linear_layer_2 = model.linear2\n",
    "relu_layer = model.relu\n",
    "\n",
    "linear_layer_1, linear_layer_2, relu_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Module.apply()\n",
    "\n",
    "The `Module.apply` function in PyTorch is a convenient method that applies a given function to each submodule recursively. It traverses the entire module hierarchy and applies the specified function to each submodule within the module. This can be useful for various tasks such as initialization, modification, or inspection of the parameters and layers within a model.\n",
    "\n",
    "In this example, the `custom_weights_initialization` function is applied to all the linear layers in the MyModel class using model.apply(custom_weights_initialization). The purpose of this is to customize the initialization of the weights of the linear layers. In this case, we are using Xavier normal initialization for the weights of linear layers.\n",
    "\n",
    "The Module.apply function is particularly useful when you want to perform some operation on each submodule, like modifying parameters, initializing weights, or any other task that involves traversing the structure of a neural network. It provides a clean and concise way to apply a function to all submodules within a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tx/kp29vzg523qdz2m7wmtms4p40000gn/T/ipykernel_1241/4228334336.py:12: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(module.weight)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "#Function to initialize the weights of linear layers with custom initialization\n",
    "def custom_weights_initialization(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_normal(module.weight)\n",
    "\n",
    "model = MyModel()\n",
    "model.apply(custom_weights_initialization)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other examples for the `Module.apply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Parameter Inspection or Modification\n",
    "\n",
    "To inspect or modify the parameters of certain layers based on some condition. For instance, to freeze the parameters of specific layers during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freeze_parameters(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.requires_grad = False\n",
    "        m.bias.requires_grad = False\n",
    "\n",
    "model.apply(freeze_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Logging or Printing information\n",
    "\n",
    "To print or log information about the layers in your model, such as the shapes of input and output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Linear, Input Shape: 10, Output Shape: 5\n",
      "Layer: Linear, Input Shape: 5, Output Shape: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_information(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(f\"Layer: {m.__class__.__name__}, Input Shape: {m.in_features}, Output Shape: {m.out_features}\")\n",
    "\n",
    "model.apply(log_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Custom Layer Operations\n",
    "\n",
    "A custom layer or operation that needs to be applied to specific types of layers in your model. #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_operation(m):\n",
    "#     if isinstance(m, MyCustomLayer):\n",
    "#         m.custom_function()\n",
    "\n",
    "# model.apply(custom_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Learning Rate Scheduling\n",
    "\n",
    "To implement a custom learning rate scheduling strategy for specific layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_learning_rate = 0.001\n",
    "epoch = 70\n",
    "def adjust_learning_rate(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        new_learning_rate = original_learning_rate * (0.1 ** (epoch//10))\n",
    "        m.optimizer.param_groups[0]['lr'] = new_learning_rate\n",
    "\n",
    "model.apply(adjust_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Model Structure modification\n",
    "\n",
    "Dynamically modify the structure of your model based on certain conditions or parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(\n",
       "    in_features=10, out_features=5, bias=True\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=5, out_features=2, bias=True\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_drop_out_layers(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.add_module('dropout', nn.Dropout(0.5))\n",
    "\n",
    "model.apply(add_drop_out_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 `bfloat16()`\n",
    "\n",
    "Casts all floating point parameters and buffers to bfloat16 datatype. #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 `buffers(recurse=True)`\n",
    "\n",
    "Used to retrieve the buffers of a `nn.Module` and its submodules. Buffers in PyTorch are a way to store persistent state that is not a parameter. They are typically used for non-learnable and non-trainable quantities.\n",
    "\n",
    "- `Buffers`: Buffers in PyTorch are attributes that are not considered parameters but still persist within the model. They might store information relevant for inference or any other non-learnable state.\n",
    "- `buffers(recurse=True)`: This method is used to get an iterator over all the buffers in a module, including buffers in its submodules if recurse=True.\n",
    "\n",
    "In this example, the MyModel class has two buffers (running_mean and running_var). The buffers(recurse=True) method is then used to iterate over all the buffers in the model, including those in its submodules. This can be useful when you want to inspect or manipulate non-learnable parameters across your entire model.\n",
    "\n",
    "The purpose of using buffers, and subsequently the buffers method, includes:\n",
    "\n",
    "i. Maintaining State: Buffers are useful for maintaining state information that is not a learnable parameter. For example, in batch normalization layers, running mean and running variance are typically stored as buffers.\n",
    "\n",
    "ii. Inspection and Modification: The buffers method allows you to iterate over buffers, inspect their values, and potentially modify them. This can be useful for tasks like resetting or updating certain states during training or inference.\n",
    "\n",
    "iii. Serialization and Saving: Buffers are included when saving and loading models. They are part of the model's persistent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Values: tensor([0., 0., 0., 0., 0.])\n",
      "Buffer Values: tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(5))\n",
    "        self.register_buffer('running_var', torch.ones(5))\n",
    "\n",
    "        self.layer = nn.Linear(5, 2)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "\n",
    "\n",
    "for buffer in model.buffers(recurse=True):\n",
    "    print(f\"Buffer Values: {buffer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 `children()`\n",
    "\n",
    "This is used to get an iterator over the immediate child modules in a nn.Module. It returns an iterator that yields pairs containing the name and module of each immediate child.\n",
    "\n",
    "- `Children`: In the context of PyTorch modules, \"children\" refer to immediate submodules contained within a module.\n",
    "- `children()`: This method is used to get an iterator over the immediate child modules of a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child Module : Linear(in_features=10, out_features=5, bias=True)\n",
      "Child Module : ReLU()\n",
      "Child Module : Linear(in_features=5, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Submodules\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(5, 2)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "for child in model.children():\n",
    "    print(f\"Child Module : {child}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of using children() includes:\n",
    "\n",
    "i. `Inspection and Modification`: You can use children() to iterate over immediate child modules for inspection or modification. For example, you might want to freeze the parameters of specific layers or apply a certain operation to each submodule.\n",
    "\n",
    "ii. `Dynamic Model Construction`: If you are dynamically constructing or modifying your model based on certain conditions, children() allows you to traverse the immediate child modules for such operations.\n",
    "\n",
    "iii. `Layer-wise Operations`: You may want to perform layer-wise operations, and children() provides a convenient way to access individual layers within your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.children():\n",
    "    if isinstance(child, nn.Linear):\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, it freezes the parameters of all linear layers within the model by setting requires_grad to False. This demonstrates how children() can be used for layer-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
